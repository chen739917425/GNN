### K-means

基于距离的数据划分算法

#### 算法过程

* 1）设定聚类中心的个数$K$，初始化$K$个样本$\{a_1,a_2,\cdots,a_K\}$，作为聚类中心
* 2）对于数据集中的每个样本$x_i$，寻找距离它最近的聚类中心，并将它分到该聚类中心对应的簇中
* 3）对于每个簇$C_i$，重新计算其聚类中心$a_i=\frac{1}{|C_i|}\sum_{x_i \in C_i}x_i$（即该簇的样本质心）
* 4）重复2），3）步骤，直到达到终止条件

#### 缺点

* $K$需要人工设定，不同的$K$值导致结果不同
* 聚类中心的初始化方式容易影响最终结果
* 易受异常样本影响
* 不适合样本太过离散、样本类别不平衡的分类任务

#### 改进

##### 数据预处理

对数据进行归一化、标准化、异常点检测

##### 合理选择$K$值

* 手肘法
  * 人工根据损失随着$K$的增大而减小的图像，选择拐点处作为最佳$K$值
* Gap statistic
  * $Gap(K)=E(logD_K)-logD_K$
  * $D_k$为损失，$E(logD_K)$为$logD_k$的期望值，可通过模拟产生
  * $Gap(K)$取得最大值处的$K$为最佳取值

##### 核K-means

采用核函数，将样本映射到更高维的空间中进行聚类

##### K-means++

改进了**K-means**对聚类中心初始化的方式

* 1）随机选取一个样本$a_1$作为聚类中心
* 2）对于每个样本$x_i$，计算目前离它最近的聚类中心的距离$D(x_i)$
* 3）对于每个样本$x_i$以概率$P(x_i)=\frac{D(x_i)^2}{\sum_{x\in X} D(x)^2}$选出下一个聚类中心，加入聚类中心集合
* 4）重复2），3）步骤，直到选出$K$个聚类中心，即完成了聚类中心的初始化

