# Population Based Training of Neural Networks  

## 背景

神经网络在许多机器学习领域都表现出色，尤其是在强化学习和监督学习中。

但一个特定的神经网络模型的表现好坏，往往取决于模型的结构，数据的表示，优化的细节等等因素的联合调整，其中每个环节都由一些超参数控制

超参数的调整会影响学习的过程，也关系着模型的表现

通常这些超参需要凭经验预先设计，需要耗费大量的精力和时间去调试，有时并不能取得好的效果

特别是在强化学习中，理想的超参数往往是极度不固定的，需要随着环境的变化而调整，而不能事先确定

常见的两种调参方式有**并行搜索**和**串行优化**

* 并行搜索是指并行地运行多个模型优化程序（即神经网络的训练），这些程序使用不同的超参数训练神经网络，从这些程序中选择一个效果最好的
* 串行优化是指串行地多次运行模型优化程序，每次调整超参数可以使用之前运行得到的信息，常见的例子就是手工调参。这种方法最终往往能获得较好的解，但面对训练耗时比较大的模型，这种方式就不太可行

## 本文工作

### 任务

机器学习的常见形式是优化模型$f$的参数$\theta$，来最大化给定的目标函数$\hat{\mathcal{Q}}$（如分类，重构，预测等任务）

但一般来说，我们关心的模型真实性能$\mathcal{Q}$（例如模型在验证集上的准确率等）与$\hat{\mathcal{Q}}$（例如模型输出的预测标签与真实标签的交叉熵损失）是有差别的

我们定义函数$eval$来计算$Q$，并忽略模型除$\theta$以外的细节，则模型的优化任务如下
$$
\theta^*=argmax_{\theta\in\Theta}eval(\theta)
$$
即寻找模型参数$\theta^*$来使得模型性能$Q$最大

若采用迭代的方式优化模型，在给定超参$h$的情况下，参数$\theta$在每次迭代步骤后被更新，即
$$
\theta\gets step(\theta|h)
$$
假设整个优化过程有$T$次迭代，$\theta$的寻优可以如下表示
$$
\theta^*=optimise(\theta|\mathbf{h})=optimise(\theta|(h_t)_{t=1}^T)=step(step(\cdots step(step(\theta|h_1)|h_2)\cdots)|h_T)
$$

其中$\mathbf{h}=\{h_1,h_2,\cdots,h_T\}$是一个关于迭代次数的参数序列

我们一般设置$h_t$都相同

### Population Based Training(PBT)

本文提出的**Population Based Training(PBT)**主要意图是提供一种方法，来协同优化可训练参数$\theta$和超参数$h$，从而获得更好的$\mathcal{Q}$

