# Actor-Critic Algorithms

## 背景

此前的强化学习方法大致落入以下两类

#### Actor-only 方法

此类方法参数化策略，通过模拟的方式采样对应动作的奖励，从而进行梯度估计，然后对策略参数进行梯度优化

这种方法的一个缺点是梯度估计的方差较大；另一方面如果策略被更新，新的梯度估计独立于过去的采样估计，没有很好地利用旧信息

#### Critic-only 方法

此类方法依赖于对价值函数的近似，学习bellman方程的近似解，然后制定一个近似最优策略

这种方法不直接优化策略，在策略的近似最优性上缺乏保证

