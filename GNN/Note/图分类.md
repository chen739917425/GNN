# 图分类任务

## 基于全局池化的图分类

使用读出机制，对$K$轮迭代后的所有节点表示进行一次聚合，得到图的全局表示
$$
\mathbf{y}=R(\{h_i^{(K)}|v_i \in V\})
$$

* 读出机制可以选取$Sum,Mean,Max$等常见操作

* 读出机制丢失了图的结构信息，因此比较适合小图数据

## 基于层次化池化的图分类

### 基于图坍缩的池化机制

#### 图坍缩

将图$G$通过某种策略划分为$K$个子图$\{\mathcal{G}^{(k)}\}$，$N_k$表示子图$\mathcal{G}^{(k)}$的节点个数，$\Gamma^{(k)}$表示子图$\mathcal{G}^{(k)}$的节点列表

每个子图（簇）为可看作一个超级节点

簇分配矩阵$S \in R^{N\times K}$
$$
S_{ij}=\begin{cases}1&if\quad v_i\in \Gamma^{(j)}\\0&otherwise\end{cases}
$$

矩阵$A_{coar}$描述了超级节点之间的连接强度
$$
A_{coar}=S^TAS
$$
采样算子$C\in R^{N\times N_k}$
$$
C_{ij}^{(k)}=\begin{cases}1&if\quad \Gamma^{(k)}_j=v_i\\0&otherwise\end{cases}
$$

设图信号$X\in R^{N\times d}$，$N$为节点个数，$d$为特征维度

图信号在子图$\mathcal{G}^{(k)}$上采样得到子图信号$X^{(k)}\in R^{N_k\times d}$
$$
X^{(k)}=(C^{(k)})^TX
$$
子图信号在全图进行上采样得到$\bar{X}^{(k)}\in R^{N\times d}$
$$
\bar{X}^{(k)}=C^{(k)}X^{(k)}
$$

* $\bar{X}^{(k)}$中，子图$\mathcal{G}^{(k)}$中节点的信号值不变，其余节点的信号置$0$

使用采样算子$C^{(k)}$可以得到子图$\mathcal{G}^{(k)}$内部的邻接矩阵$A^{(k)}\in R^{N_k\times N_k}$
$$
A^{(k)}=(C^{(k)})^TAC^{(k)}
$$
通过$(3)$与$(7)$可以分别获得子图间和子图内的邻接关系

对每个子图，利用子图内的邻接关系，融合内部节点的信号，将结果作为该子图对应的超级节点的信号，再根据子图间的邻接关系，得到一张超级节点构成的新图

不断迭代上述过程，图最后坍缩成一个超级节点，就获得了全局的图信号



#### DIFFPOOL

提出一个可学习的簇分配矩阵，将图坍缩和GNN结合起来进行图层面的学习

$GNN_{l,embed}$进行节点的特征学习，$GNN_{l,pool}$学习每个节点分配到各个簇的概率
$$
Z^{(l)}=GNN_{l,embed}(A^{(l)},H^{(l)})
$$

$$
S^{(l)}=softmax(GNN_{l,pool}(A^{(l)},H^{(l)}))
$$

* 其中，$A^{(l)}\in R^{n^{(l)}\times n^{(l)}},S^{(l)}\in R^{n^{(l)}\times n^{(l+1)}}$，$n^{(l)}$表示第$l$层的节点数
* $S$是一个软分配器，表示每个节点被分到每个簇的概率，因此是一个当前层所有节点到下一层所有超级节点的全连接结构
* 为了最后将图坍缩为一个超级节点，得到全局表示，最后一层的簇分配矩阵固定为一个全$\mathbf{1}$矩阵

使用$(8),(9)$的结果对图进行坍缩
$$
H^{(l+1)}=S^{(l)^T}Z^{(l)}
$$

$$
A^{(l+1)}=S^{(l)^T}A^{(l)}S^{(l)}
$$

* 上述两式即为**DIFFPOOL**层
* 式$(10)$对所有节点的特征表示进行加和，进行了融合的操作，得到下一层新图上的节点特征表示
* 式$(11)$是下一层新图上的邻接矩阵

**DIFFPOOL**具有排列不变性



#### EigenPooling

**EigenPooling**没有引入任何学习参数

### 基于TopK机制的池化机制



### 基于边收缩的池化机制





